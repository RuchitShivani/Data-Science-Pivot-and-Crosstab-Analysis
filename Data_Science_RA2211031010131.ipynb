{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeEaJ9BNysRTcibrYqPFO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuchitShivani/Data-Science-Pivot-and-Crosstab-Analysis/blob/main/Data_Science_RA2211031010131.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING EMPLOYEE DATASET"
      ],
      "metadata": {
        "id": "SKX7U5CDVpVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QGWCw-qTpQP",
        "outputId": "1cfc47d6-d238-4a2f-ea1a-14b7418a5117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ employee.csv generated!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Sample values\n",
        "names = ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank', 'Grace', 'Helen', 'Ian', 'Jack']\n",
        "countries = ['India', 'USA', 'Germany', 'France', 'Canada']\n",
        "departments = ['HR', 'Tech', 'Finance', 'Marketing']\n",
        "positions = ['Manager', 'Executive', 'Analyst', 'Intern']\n",
        "\n",
        "# Generate sample employee data\n",
        "employee_data = []\n",
        "for i in range(1, 21):  # 20 records\n",
        "    name = random.choice(names)\n",
        "    age = random.randint(22, 60)\n",
        "    country = random.choice(countries)\n",
        "    department = random.choice(departments)\n",
        "    position = random.choice(positions)\n",
        "    salary = round(random.uniform(30000, 150000), 2)\n",
        "    join_date = pd.Timestamp('2015-01-01') + pd.to_timedelta(random.randint(0, 3000), unit='D')\n",
        "    employee_data.append([i, name, age, country, department, position, salary, join_date])\n",
        "\n",
        "# Create DataFrame\n",
        "df_employee = pd.DataFrame(employee_data, columns=[\n",
        "    'Employee_ID', 'Employee_Name', 'Age', 'Country', 'Department',\n",
        "    'Position', 'Salary', 'Joining_Date'\n",
        "])\n",
        "\n",
        "# Introduce missing values (optional for analysis)\n",
        "df_employee.loc[3, 'Salary'] = np.nan\n",
        "df_employee.loc[7, 'Salary'] = np.nan\n",
        "\n",
        "# Save as CSV\n",
        "df_employee.to_csv(\"employee.csv\", index=False)\n",
        "print(\"✅ employee.csv generated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING CROP DATASET"
      ],
      "metadata": {
        "id": "TcsMnuyKVusy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crops = ['Rice', 'Wheat', 'Maize', 'Soybean', 'Cotton']\n",
        "seasons = ['Monsoon', 'Winter', 'Summer']\n",
        "regions = ['North', 'South', 'East', 'West']\n",
        "fertilizer_used = ['Yes', 'No']\n",
        "\n",
        "crop_data = []\n",
        "for _ in range(50):\n",
        "    crop = random.choice(crops)\n",
        "    year = random.randint(2015, 2023)\n",
        "    season = random.choice(seasons)\n",
        "    region = random.choice(regions)\n",
        "    area = round(random.uniform(100, 1000), 2)\n",
        "    production = round(area * random.uniform(0.8, 1.5), 2)\n",
        "    rainfall = round(random.uniform(400, 1200), 2)\n",
        "    fertilizer = random.choice(fertilizer_used)\n",
        "    pesticide = random.choice(['Yes', 'No'])\n",
        "    yield_val = round(production / area, 2)\n",
        "    crop_data.append([region, crop, year, season, area, production, rainfall, fertilizer, pesticide, yield_val])\n",
        "\n",
        "df_crop = pd.DataFrame(crop_data, columns=[\n",
        "    'Region', 'Crop', 'Crop_Year', 'Season', 'Area',\n",
        "    'Production', 'Annual_Rainfall', 'Fertilizer', 'Pesticide', 'Yield'\n",
        "])\n",
        "\n",
        "df_crop.to_csv(\"crop_yield.csv\", index=False)\n",
        "print(\"✅ crop_yield.csv generated!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYjvWH1fT-rz",
        "outputId": "3a67f515-b83d-4451-acbf-a3d76dcd2330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ crop_yield.csv generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING CUSTOMER DATASET"
      ],
      "metadata": {
        "id": "WhjlL5WMVy_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker\n",
        "\n",
        "import faker\n",
        "\n",
        "fake = faker.Faker()\n",
        "customers = []\n",
        "\n",
        "for i in range(1, 21):\n",
        "    cust_id = i if random.random() > 0.1 else None  # 10% missing\n",
        "    name = fake.name()\n",
        "    email = fake.email() if random.random() > 0.2 else 'invalid_email'\n",
        "    phone = fake.phone_number()\n",
        "    purchase = round(random.uniform(100, 10000), 2)\n",
        "    if random.random() < 0.1:\n",
        "        purchase = -100  # Invalid negative\n",
        "    elif random.random() < 0.1:\n",
        "        purchase = 999999  # Unrealistic\n",
        "\n",
        "    customers.append([cust_id, name, email, phone, purchase])\n",
        "\n",
        "df_cust = pd.DataFrame(customers, columns=[\n",
        "    'Customer ID', 'Name', 'Email', 'Phone', 'Purchase Amount'\n",
        "])\n",
        "\n",
        "df_cust.to_csv(\"customer.csv\", index=False)\n",
        "print(\"✅ customer.csv generated!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_5x8IgSUBMV",
        "outputId": "d828ff5d-d573-4465-89ec-4e2a1ca9d9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.1.0\n",
            "✅ customer.csv generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " EMPLOYEE DATASET ANALYSIS(Q1)"
      ],
      "metadata": {
        "id": "ya7RmZ9IUU1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('employee.csv')\n",
        "\n",
        "print(\"Rows and Columns:\", df.shape)\n",
        "\n",
        "print(\"\\nDescriptive Statistics:\\n\", df.describe(include='all'))\n",
        "\n",
        "print(\"\\nRows with missing Salary values:\\n\", df[df['Salary'].isna()])\n",
        "\n",
        "print(\"\\nFirst 4 rows:\\n\", df.head(4))\n",
        "\n",
        "# v. Select specific columns and rows (e.g., Name, Salary of first 5 entries)\n",
        "print(\"\\nSelected Columns and Rows:\\n\", df.loc[0:4, ['Employee_Name', 'Salary']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF6OSoEPURxJ",
        "outputId": "de9dd4b8-22b6-4d6b-e83d-e8efb77c2600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows and Columns: (20, 8)\n",
            "\n",
            "Descriptive Statistics:\n",
            "         Employee_ID Employee_Name        Age Country Department Position  \\\n",
            "count      20.00000            20  20.000000      20         20       20   \n",
            "unique          NaN             8        NaN       5          4        4   \n",
            "top             NaN         Alice        NaN     USA         HR  Analyst   \n",
            "freq            NaN             4        NaN       7          7        7   \n",
            "mean       10.50000           NaN  46.600000     NaN        NaN      NaN   \n",
            "std         5.91608           NaN   8.720212     NaN        NaN      NaN   \n",
            "min         1.00000           NaN  23.000000     NaN        NaN      NaN   \n",
            "25%         5.75000           NaN  44.250000     NaN        NaN      NaN   \n",
            "50%        10.50000           NaN  48.000000     NaN        NaN      NaN   \n",
            "75%        15.25000           NaN  52.000000     NaN        NaN      NaN   \n",
            "max        20.00000           NaN  60.000000     NaN        NaN      NaN   \n",
            "\n",
            "               Salary Joining_Date  \n",
            "count       18.000000           20  \n",
            "unique            NaN           20  \n",
            "top               NaN   2023-01-04  \n",
            "freq              NaN            1  \n",
            "mean     76288.771667          NaN  \n",
            "std      36415.758589          NaN  \n",
            "min      32320.650000          NaN  \n",
            "25%      45108.290000          NaN  \n",
            "50%      72172.220000          NaN  \n",
            "75%      98915.440000          NaN  \n",
            "max     142939.930000          NaN  \n",
            "\n",
            "Rows with missing Salary values:\n",
            "    Employee_ID Employee_Name  Age Country Department   Position  Salary  \\\n",
            "3            4         Alice   58     USA         HR  Executive     NaN   \n",
            "7            8         Alice   42   India         HR     Intern     NaN   \n",
            "\n",
            "  Joining_Date  \n",
            "3   2015-06-09  \n",
            "7   2021-04-25  \n",
            "\n",
            "First 4 rows:\n",
            "    Employee_ID Employee_Name  Age Country Department   Position     Salary  \\\n",
            "0            1         Alice   48   India       Tech    Analyst   91846.35   \n",
            "1            2          Jack   30  France    Finance  Executive   99904.08   \n",
            "2            3         Alice   53     USA         HR     Intern  111980.30   \n",
            "3            4         Alice   58     USA         HR  Executive        NaN   \n",
            "\n",
            "  Joining_Date  \n",
            "0   2023-01-04  \n",
            "1   2019-06-09  \n",
            "2   2015-08-25  \n",
            "3   2015-06-09  \n",
            "\n",
            "Selected Columns and Rows:\n",
            "   Employee_Name     Salary\n",
            "0         Alice   91846.35\n",
            "1          Jack   99904.08\n",
            "2         Alice  111980.30\n",
            "3         Alice        NaN\n",
            "4         David   92301.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AGRICULTURE DATASET GROUP-BY, PIVOT, CROSSTAB(Q2,Q3)"
      ],
      "metadata": {
        "id": "2isfgPoAUcXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_crop = pd.read_csv('crop_yield.csv')\n",
        "\n",
        "grouped = df_crop.groupby(['Region', 'Season'])['Crop'].value_counts().unstack().fillna(0)\n",
        "print(\"\\nCrop counts by Region and Season:\\n\", grouped)\n",
        "\n",
        "pivot = pd.pivot_table(df_crop, values='Production', index=['Region', 'Season'],\n",
        "                       columns='Crop', aggfunc='sum', fill_value=0)\n",
        "print(\"\\nPivot Table - Total Production:\\n\", pivot)\n",
        "\n",
        "crosstab = pd.crosstab(df_crop['Crop'], df_crop['Fertilizer'])\n",
        "print(\"\\nCrosstab of Crop and Fertilizer Usage:\\n\", crosstab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnPhhtViUc3w",
        "outputId": "e37dbead-2e75-4d16-997a-57e8abcdae53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Crop counts by Region and Season:\n",
            " Crop            Cotton  Maize  Rice  Soybean  Wheat\n",
            "Region Season                                      \n",
            "East   Monsoon     0.0    1.0   0.0      2.0    1.0\n",
            "       Summer      1.0    3.0   0.0      3.0    1.0\n",
            "       Winter      0.0    1.0   1.0      0.0    0.0\n",
            "North  Monsoon     2.0    2.0   0.0      0.0    4.0\n",
            "       Summer      2.0    0.0   0.0      1.0    2.0\n",
            "       Winter      0.0    2.0   0.0      2.0    0.0\n",
            "South  Monsoon     2.0    0.0   1.0      1.0    1.0\n",
            "       Summer      0.0    0.0   3.0      0.0    0.0\n",
            "       Winter      1.0    0.0   2.0      0.0    0.0\n",
            "West   Monsoon     0.0    0.0   1.0      1.0    1.0\n",
            "       Summer      1.0    0.0   0.0      2.0    0.0\n",
            "       Winter      1.0    1.0   0.0      0.0    0.0\n",
            "\n",
            "Pivot Table - Total Production:\n",
            " Crop             Cotton    Maize     Rice  Soybean    Wheat\n",
            "Region Season                                              \n",
            "East   Monsoon     0.00  1257.23     0.00  1364.36   893.89\n",
            "       Summer    720.10   816.37     0.00  2305.06  1270.01\n",
            "       Winter      0.00   898.99  1079.50     0.00     0.00\n",
            "North  Monsoon  1144.30   741.23     0.00     0.00  2921.95\n",
            "       Summer    813.13     0.00     0.00   593.86  1411.42\n",
            "       Winter      0.00  1989.64     0.00  2330.20     0.00\n",
            "South  Monsoon   610.22     0.00   547.47   778.66   505.15\n",
            "       Summer      0.00     0.00  2455.73     0.00     0.00\n",
            "       Winter    400.13     0.00  1078.85     0.00     0.00\n",
            "West   Monsoon     0.00     0.00   461.08   382.90   453.61\n",
            "       Summer    310.29     0.00     0.00  1205.77     0.00\n",
            "       Winter    703.80  1225.80     0.00     0.00     0.00\n",
            "\n",
            "Crosstab of Crop and Fertilizer Usage:\n",
            " Fertilizer  No  Yes\n",
            "Crop               \n",
            "Cotton       3    7\n",
            "Maize        5    5\n",
            "Rice         5    3\n",
            "Soybean     10    2\n",
            "Wheat        6    4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUSTOMER DATA CLEANING(Q4)"
      ],
      "metadata": {
        "id": "cunWJFOSUh7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df_cust = pd.read_csv('customer.csv')\n",
        "\n",
        "df_cust['Customer ID'] = df_cust['Customer ID'].fillna(-1).astype(int)\n",
        "\n",
        "df_cust['Name'] = df_cust['Name'].str.title()\n",
        "\n",
        "df_cust['Email_Valid'] = df_cust['Email'].apply(lambda x: re.match(r\"[^@]+@[^@]+\\.[^@]+\", str(x)) is not None)\n",
        "\n",
        "df_cust['Phone'] = df_cust['Phone'].str.replace(r'\\D', '', regex=True)\n",
        "\n",
        "def clean_purchase(val):\n",
        "    if val < 0 or val > 50000:\n",
        "        return np.nan  \\\n",
        "    return val\n",
        "\n",
        "df_cust['Purchase Amount'] = df_cust['Purchase Amount'].apply(clean_purchase)\n",
        "\n",
        "\n",
        "print(\"\\nCleaned Customer Dataset:\\n\", df_cust.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv1mBrpAUZYv",
        "outputId": "bdd74350-a14b-49aa-b2fd-d659e742ae24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned Customer Dataset:\n",
            "    Customer ID           Name                     Email               Phone  \\\n",
            "0            1  Angel Flowers  shaneschultz@example.com     179153095445622   \n",
            "1            2  Lori Thompson             invalid_email  001355827265722221   \n",
            "2            3    Sara Oliver  philiphughes@example.org    1992615658069382   \n",
            "3            4  Victor Thomas             invalid_email          4493123694   \n",
            "4            5   Joshua Henry    michelle01@example.net          7673268524   \n",
            "\n",
            "   Purchase Amount  Email_Valid  \n",
            "0              NaN         True  \n",
            "1          2814.81        False  \n",
            "2          6092.91         True  \n",
            "3          5818.07        False  \n",
            "4          9217.35         True  \n"
          ]
        }
      ]
    }
  ]
}